---
title: "Computational Intelligence - Assignment 1"
author: "Bruno Sánchez and Sheena Lang"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction

This report describes our experiment in training a neural network (NN) using backpropagation, genetic algorithms (GA), and evolutionary strategies (ES), and evaluating these methods on the testing dataset.

We begin by outlining the data collection process, using synthetic data generation based on the real Iris dataset.

Next, we detail each of the three training approaches (backpropagation, GA, and ES) and conduct a hyperparameter search for GA and ES.

Finally, we assess the accuracy and training time of these methods under different synthetic data conditions (hardness, noise, and training size) and present a statistical analysis of the results to identify any significant differences.

## Data Collection

We aim to generate synthetic data based on the Iris dataset, a widely used benchmark in machine learning, to maintain a connection to a real-world problem while still allowing for flexibility. 

By creating synthetic data instead of using the original Iris dataset, we can control the hardness of the classification task and adjust the level of noise, enabling a more flexible experimental setup.

### Analysis Iris Dataset

The Iris dataset consists of flower samples from three different species: Setosa, Versicolor, and Virginica. Each sample includes measurements of four features: sepal length, sepal width, petal length, and petal width.

To ensure that our synthetic data accurately simulates Iris samples, we first perform a preliminary analysis to calculate the feature means and covariances for each of the three species.

The following code chunk calculates and displays these statistics:

### Synthetic Data Generation

By using the means and covariances of the three flower species, we generate synthetic samples by drawing data points from distributions defined by these statistical properties.

To introduce flexibility, we add parameters to control two aspects: *hardness* and *noise* of the data.

- *Hardness* controls the separation between species, with higher values making the classification task easier. Increasing hardness involves expanding the distance between the species’ feature distributions, making them more distinct and easier to classify.

- *Noise* adds random variation to the data, making the classification task more challenging. A higher noise parameter increases overlap between the species’ distributions by introducing variability to feature values. This reduces the distinctiveness of the species and makes it harder for the model to classify them correctly.

These parameters allow us to simulate a range of classification challenges, from well-separated species with minimal noise (easy classification) to highly overlapping distributions with high noise (difficult classification).


```{r}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(GGally))


generate_synthetic_data <- function(n_train, n_val, n_test, noise_level = 0.1, hardness = 0.5) {
  set.seed(123)

  mu_setosa <- c(5.01, 3.43, 1.46, 0.246)
  mu_versicolor <- c(5.94, 2.77, 4.26, 1.33)
  mu_virginica <- c(6.59, 2.97, 5.55, 2.03)

  mu_setosa <- mu_setosa - c(hardness, hardness, hardness, hardness)
  mu_virginica <- mu_virginica + c(hardness, hardness, hardness, hardness)

  setosa_cov <- matrix(c(
    0.12424898, 0.099216327, 0.016355102, 0.010330612,
    0.09921633, 0.143689796, 0.011697959, 0.009297959,
    0.01635510, 0.011697959, 0.030159184, 0.006069388,
    0.01033061, 0.009297959, 0.006069388, 0.011106122
  ), nrow = 4, byrow = TRUE)

  versicolor_cov <- matrix(c(
    0.26643265, 0.08518367, 0.18289796, 0.05577959,
    0.08518367, 0.09846939, 0.08265306, 0.04120408,
    0.18289796, 0.08265306, 0.22081633, 0.07310204,
    0.05577959, 0.04120408, 0.07310204, 0.03910612
  ), nrow = 4, byrow = TRUE)

  virginica_cov <- matrix(c(
    0.40434286, 0.09376327, 0.30328980, 0.04909388,
    0.09376327, 0.10400408, 0.07137959, 0.04762857,
    0.30328980, 0.07137959, 0.30458776, 0.04882449,
    0.04909388, 0.04762857, 0.04882449, 0.07543265
  ), nrow = 4, byrow = TRUE)

  X_setosa_train <- mvrnorm(n_train %/% 3, mu_setosa, setosa_cov)
  X_setosa_val <- mvrnorm(n_val %/% 3, mu_setosa, setosa_cov)
  X_setosa_test <- mvrnorm(n_test %/% 3, mu_setosa, setosa_cov)

  X_versicolor_train <- mvrnorm(n_train %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_val <- mvrnorm(n_val %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_test <- mvrnorm(n_test %/% 3, mu_versicolor, versicolor_cov)

  X_virginica_train <- mvrnorm(n_train %/% 3, mu_virginica, virginica_cov)
  X_virginica_val <- mvrnorm(n_val %/% 3, mu_virginica, virginica_cov)
  X_virginica_test <- mvrnorm(n_test %/% 3, mu_virginica, virginica_cov)

  X_train <- rbind(X_setosa_train, X_versicolor_train, X_virginica_train)
  y_train <- c(rep(0, n_train %/% 3), rep(1, n_train %/% 3), rep(2, n_train %/% 3))

  X_val <- rbind(X_setosa_val, X_versicolor_val, X_virginica_val)
  y_val <- c(rep(0, n_val %/% 3), rep(1, n_val %/% 3), rep(2, n_val %/% 3))

  X_test <- rbind(X_setosa_test, X_versicolor_test, X_virginica_test)
  y_test <- c(rep(0, n_test %/% 3), rep(1, n_test %/% 3), rep(2, n_test %/% 3))

  X_train <- X_train + rnorm(length(X_train), 0, noise_level)
  X_val <- X_val + rnorm(length(X_val), 0, noise_level)
  X_test <- X_test + rnorm(length(X_test), 0, noise_level)

  list(
    train = data.frame(X_train, y_train = as.factor(y_train)),
    val = data.frame(X_val, y_val = as.factor(y_val)),
    test = data.frame(X_test, y_test = as.factor(y_test))
  )
}

# hardness_values <- c(0.2, 1, 2)
# noise_levels <- c(0.2, 1, 2)
# training_sizes <- c(100, 1000, 10000)
# validation_size <- 1000
# test_size <- 1000

# for (training_size in training_sizes){
#     for (noise_level in noise_levels) {
#         for (hardness in hardness_values) {
#             synthetic_data <- generate_synthetic_data(training_size, validation_size, test_size, noise_level, hardness)
#             plot <-ggpairs(synthetic_data$train, aes(color = y_train)) +
#             ggtitle(paste("Pair Plot of Synthetic Training Data (Noise:", noise_level, ", Hardness:", hardness, ")"))
#             print(plot)
#   }
# }
# }
```

## Neural Network

- Explain how it was implemented

```{r}
suppressPackageStartupMessages(library(nnet))

train_bp <- function(train_data, size, maxit, specific_params) {
  decay <- specific_params$decay
  train_data$y_train <- as.factor(train_data$y_train)

  model <- nnet(y_train ~ .,
    data = train_data,
    size = size,
    maxit = maxit,
    decay = decay,
    linout = FALSE,
    trace = FALSE
  )

  return(model)
}

test <- function(model, test_data) {
  y_pred <- predict(model, newdata = test_data[, -ncol(test_data)], type = "class")

  accuracy <- accuracy <- mean(y_pred == test_data[, ncol(test_data)])

  # cat(sprintf("Test Accuracy: %.4f\n", accuracy))

  return(accuracy)
}
```

## Genetic Algorithm
- Explain how it was implemented


```{r}
suppressPackageStartupMessages(library(GA))


train_ga <- function(train_data, size, maxit, specific_params) {
  pcrossover <- specific_params$pcrossover
  pmutation <- specific_params$pmutation
  popSize <- specific_params$pop_size_ga

  min_size <- 1
  max_size <- size

  n_params <- 5 * max_size + (max_size + 1) * 3

  # Set lower and upper bounds for the weights
  lower <- c(min_size, rep(-1, n_params))
  upper <- c(max_size, rep(1, n_params))

  # Set up GA parameters with filled lower and upper bounds
  ga_model <- ga(
    type = "real-valued",
    fitness = fitness_function,
    lower = lower,
    upper = upper,
    pcrossover = pcrossover,
    pmutation = pmutation,
    popSize = popSize,
    maxiter = maxit,
    run = 50,
    monitor = FALSE
  )
  # Retrieve optimized weights
  # Returns all of the individuals that are tied for the best fitness score, we only keep the first
  optimal_weights <- ga_model@solution[1, ]

  optimal_size <- round(optimal_weights[1])
  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = optimal_size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

  # Set weights of the neural network
  nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

  return(nn_model)
}


fitness_function <- function(weights) {
  # Instantiate the neural network with the number of neurons given by the first gene
  size <- round(weights[1])
  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  # Set weights of the neural network
  nn_model$wts <- weights[-1]

  # Predict on training data
  predictions <- predict(nn_model, as.matrix(train_data[, -ncol(train_data)]), type = "class")

  # Calculate misclassification error
  error <- mean(predictions != train_data[, ncol(train_data)])

  # Return negative error to maximize fitness function
  return(-error)
}
```

## Hyperparameter Search GA

```{r, eval=FALSE}
hyperparam_search_ga <- function(train_data, validation_data, pcrossover_values, pmutation_values, popSize_values) {
  print("Running hyperparameter search ga...")

  best_model <- NULL
  best_score <- -Inf
  best_params <- list()

  for (pcrossover in pcrossover_values) {
    for (pmutation in pmutation_values) {
      for (popSize in popSize_values) {
        specific_params <- list(pcrossover = pcrossover, pmutation = pmutation, pop_size_ga = popSize)

        model <- train_ga(train_data, size, maxit, specific_params)

        score <- test(model, validation_data)

        if (score > best_score) {
          best_score <- score
          best_model <- model
          best_params <- list(pcrossover = pcrossover, pmutation = pmutation, popSize = popSize)
        }

        cat("Tested pcrossover =", pcrossover, "pmutation =", pmutation, "popSize =", popSize, "-> score:", score, "\n")
      }
    }
  }
  best_p_crossover_ga <- best_params$pcrossover
  best_p_mutation_ga <- best_params$pmutation
  best_pop_size_ga <- best_params$popSize


  cat("Best pcrossover:", best_p_crossover_ga, "Best pmutation:", best_p_mutation_ga, "Best popsize:", best_pop_size_ga, "Best score:", best_score, "\n")

  return(list(best_model = best_model, best_score = best_score, best_params = best_params))
}

data <- generate_synthetic_data(200, 200, 200, 0.5, 0.5)
train_data <- data$train
validation_data <- data$val

size <- 10
maxit <- 200

# Define possible hyperparameter ranges
pcrossover_values <- c(0.7, 0.8, 0.9)
pmutation_values <- c(0.05, 0.1, 0.2)
popSize_values <- c(20, 50, 100)

best_result <- hyperparam_search_ga(train_data, validation_data, pcrossover_values, pmutation_values, popSize_values)

best_pcrossover_ga <- best_result$best_params$pcrossover
best_pmutation_ga <- best_result$best_params$pmutation
best_pop_size_ga <- best_result$best_params$popSize
```

```{r, echo=FALSE}
# Hard-coded explanation

best_pcrossover_ga <- 0.7
best_pmutation_ga <- 0.2
best_pop_size_ga <- 100
```

## Evolution Strategy
- Explain how it was implemented

```{r}
suppressPackageStartupMessages(library(cmaesr))

# TODO modify optional param values
train_es <- function(train_data, size, maxit, specific_params) {
  sigma <- specific_params$sigma
  popSize <- specific_params$pop_size_es
  min_size <- 1
  max_size <- size

  n_params <- 5 * max_size + (max_size + 1) * 3

  lower <- c(min_size, rep(-1, n_params))
  upper <- c(max_size + 1, rep(1, n_params))

  obj.fn <- makeSingleObjectiveFunction(
    name = "Neural Network Fitness Function",
    fn = function(x) -fitness_function(x), # ES minimizes objective function
    par.set = makeNumericParamSet("weights", len = n_params + 1, lower = lower, upper = upper)
  )

  # Run the ES model
  suppressWarnings(
    es_model <- cmaes(
      obj.fn,
      monitor = makeMonitor(),
      control = list(
        sigma = sigma, # initial step size
        lambda = popSize, # number of offspring
        stop.ons = c(
          list(stopOnMaxIters(maxit)), # stop after maxit iterations
          getDefaultStoppingConditions() # or after default stopping conditions
        )
      )
    )
  )

  # Retrieve optimized weights
  optimal_weights <- es_model$best.param

  optimal_size <- round(optimal_weights[1])

  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = optimal_size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

  # Set weights of the neural network
  nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

  return(nn_model)
}
```

## Hyperparameter Search ES

```{r, eval=FALSE}
hyperparam_search_es <- function(train_data, validation_data, sigma_values, popSize_values) {
  best_model <- NULL
  best_score <- -Inf
  best_params <- list()

  for (sigma in sigma_values) {
    for (popSize in popSize_values) {
      specific_params <- list(sigma = sigma, pop_size_es = popSize)
      model <- train_es(train_data, size, maxit, specific_params)

      score <- test(model, validation_data)

      if (score > best_score) {
        best_score <- score
        best_model <- model
        best_params <- list(sigma = sigma, popSize = popSize)
      }

      cat("Tested sigma =", sigma, "popSize =", popSize, "-> score:", score, "\n")
    }
  }

  best_sigma_es <- best_params$sigma
  best_pop_size_es <- best_params$popSize

  cat("Best sigma:", best_sigma_es, "Best popSize:", best_pop_size_es, "Best score:", best_score, "\n")

  return(list(best_model = best_model, best_score = best_score, best_params = best_params))
}

data <- generate_synthetic_data(200, 200, 200, 0.5, 0.5)
train_data <- data$train
validation_data <- data$val

size <- 10
maxit <- 200

# Define possibel hyperparameter ranges
sigma_values <- c(0.4, 0.5, 0.6)
popSize_values <- c(20, 50, 70)

best_result <- hyperparam_search_es(train_data, validation_data, sigma_values, popSize_values)

best_sigma_es <- best_result$best_params$sigma
best_pop_size_es <- best_result$best_params$popSize
```


```{r, echo=FALSE}
# Hard-coded explanation

# TODO modify numbers
best_sigma_es <- 0.4
best_pop_size_es <- 70
```

## Experimental Setup 
- Explain setup of how evaluation is performed (Parameters we are variating)
- Highlight results in textual form

```{r, eval = FALSE}
suppressPackageStartupMessages(library(tictoc))

# ---------- DATA PARAMETERS ----------

hardness_values <- c(0.2, 2)
noise_levels <- c(0.2, 2)
training_sizes <- c(100, 500) # TODO ADD MORE
validation_size <- 1000
test_size <- 1000

# ---------- RUN PARAMETERS ----------

sizes <- c(1, 10, 20)
maxit <- 200
training_functions <- c("train_bp", "train_ga", "train_es")
specific_params <- list(decay = 0.001, pcrossover = best_pcrossover_ga, pmutation = best_pmutation_ga, pop_size_ga = best_pop_size_ga, sigma = best_sigma_es, pop_size_es = best_pop_size_es)


# ---------- RESULT STRUCTURE ----------

results <- data.frame(
  hardness = numeric(),
  noise = numeric(),
  training_size = numeric(),
  size = numeric(),
  maxit = numeric(),
  training_function = character(),
  test_accuracy = numeric(),
  training_time = numeric(),
  stringsAsFactors = FALSE
)

# ---------- EVALUATION ----------

for (hardness in hardness_values) {
  for (noise in noise_levels) {
    for (training_size in training_sizes) {
      data <- generate_synthetic_data(training_size, validation_size, test_size, noise, hardness)

      train_data <- data$train
      val_data <- data$val
      test_data <- data$test

      for (size in sizes) {
        for (training_function in training_functions) {
          # Retrieve the training function by name
          func <- get(training_function)

          # Measure training time using tic and toc
          tic()
          model <- func(train_data = train_data, size = size, maxit = maxit, specific_params = specific_params)
          training_time <- toc(quiet = TRUE)$toc

          # Test accuracy
          test_accuracy <- test(model, test_data)

          # Store result
          results <- rbind(results, data.frame(
            hardness = hardness,
            noise = noise,
            training_size = training_size,
            size = size,
            training_function = training_function,
            test_accuracy = test_accuracy,
            training_time = training_time
          ))
        }
      }
    }
  }
}

write.csv(results, "results.csv", row.names = FALSE)
```

## Results

### Results Overview

### Statistical Analysis

```{r}
suppressPackageStartupMessages(library(dplyr))

results <- read.csv("results.csv")

# Identify the fastest and most accurate runs
fastest_run <- results[which.min(results$training_time), ]
most_accurate_run <- results[which.max(results$test_accuracy), ]

# Fastest run per training size
fastest_runs <- results %>%
  group_by(training_size) %>%
  filter(training_time == min(training_time)) %>%
  ungroup()

# Most accurate run per hardness and noise
most_accurate_runs <- results %>%
  group_by(hardness, noise) %>%
  filter(test_accuracy == max(test_accuracy)) %>%
  ungroup()

# Fastest Runs per Training Size:
print(fastest_runs)

# Most Accurate Runs per Hardness and Noise:
print(most_accurate_runs)
```


## Conclusion

