---
title: "Computational Intelligence - Assignment 1"
author: "Bruno SÃ¡nchez and Sheena Lang"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction

This is an example of R Markdown.

## Data Collection

```{r}

suppressPackageStartupMessages(library(MASS))  
suppressPackageStartupMessages(library(caret)) 
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(GGally))


generate_synthetic_data <- function(n_train, n_val, n_test, noise_level = 0.1, hardness = 0.5) {
  set.seed(123)
  
  mu_setosa <- c(5.01, 3.43, 1.46, 0.246)  
  mu_versicolor <- c(5.94, 2.77, 4.26, 1.33)  
  mu_virginica <- c(6.59, 2.97, 5.55, 2.03)

  mu_setosa <- mu_setosa - c(hardness, hardness, hardness, hardness)
  mu_virginica <- mu_virginica + c(hardness, hardness, hardness, hardness) 

  setosa_cov <- matrix(c(
    0.12424898, 0.099216327, 0.016355102, 0.010330612,
    0.09921633, 0.143689796, 0.011697959, 0.009297959,
    0.01635510, 0.011697959, 0.030159184, 0.006069388,
    0.01033061, 0.009297959, 0.006069388, 0.011106122
  ), nrow = 4, byrow = TRUE)

  versicolor_cov <- matrix(c(
    0.26643265, 0.08518367, 0.18289796, 0.05577959,
    0.08518367, 0.09846939, 0.08265306, 0.04120408,
    0.18289796, 0.08265306, 0.22081633, 0.07310204,
    0.05577959, 0.04120408, 0.07310204, 0.03910612
  ), nrow = 4, byrow = TRUE)

  virginica_cov <- matrix(c(
    0.40434286, 0.09376327, 0.30328980, 0.04909388,
    0.09376327, 0.10400408, 0.07137959, 0.04762857,
    0.30328980, 0.07137959, 0.30458776, 0.04882449,
    0.04909388, 0.04762857, 0.04882449, 0.07543265
  ), nrow = 4, byrow = TRUE)  
  
  X_setosa_train <- mvrnorm(n_train %/% 3, mu_setosa, setosa_cov)
  X_setosa_val <- mvrnorm(n_val %/% 3, mu_setosa, setosa_cov)
  X_setosa_test <- mvrnorm(n_test %/% 3, mu_setosa, setosa_cov) 
  
  X_versicolor_train <- mvrnorm(n_train %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_val <- mvrnorm(n_val %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_test <- mvrnorm(n_test %/% 3, mu_versicolor, versicolor_cov) 
  
  X_virginica_train <- mvrnorm(n_train %/% 3, mu_virginica, virginica_cov)
  X_virginica_val <- mvrnorm(n_val %/% 3, mu_virginica, virginica_cov)
  X_virginica_test <- mvrnorm(n_test %/% 3, mu_virginica, virginica_cov)
  
  X_train <- rbind(X_setosa_train, X_versicolor_train, X_virginica_train)
  y_train <- c(rep(0, n_train %/% 3), rep(1, n_train %/% 3), rep(2, n_train %/% 3))
  
  X_val <- rbind(X_setosa_val, X_versicolor_val, X_virginica_val)
  y_val <- c(rep(0, n_val %/% 3), rep(1, n_val %/% 3), rep(2, n_val %/% 3))
  
  X_test <- rbind(X_setosa_test, X_versicolor_test, X_virginica_test)
  y_test <- c(rep(0, n_test %/% 3), rep(1, n_test %/% 3), rep(2, n_test %/% 3))
  
  X_train <- X_train + rnorm(length(X_train), 0, noise_level)
  X_val <- X_val + rnorm(length(X_val), 0, noise_level)
  X_test <- X_test + rnorm(length(X_test), 0, noise_level)
  
  list(
    train = data.frame(X_train, y_train = as.factor(y_train)),
    val = data.frame(X_val, y_val = as.factor(y_val)),
    test = data.frame(X_test, y_test = as.factor(y_test))
  )
}

# hardness_values <- c(0.2, 1, 2)
# noise_levels <- c(0.2, 1, 2)
# training_sizes <- c(100, 1000, 10000)
# validation_size <- 1000
# test_size <- 1000

# for (training_size in training_sizes){
#     for (noise_level in noise_levels) {
#         for (hardness in hardness_values) {
#             synthetic_data <- generate_synthetic_data(training_size, validation_size, test_size, noise_level, hardness)
#             plot <-ggpairs(synthetic_data$train, aes(color = y_train)) +
#             ggtitle(paste("Pair Plot of Synthetic Training Data (Noise:", noise_level, ", Hardness:", hardness, ")"))
#             print(plot)
#   }
# }
# }

```

## Neural Network

```{r}

suppressPackageStartupMessages(library(nnet))

train_bp <- function(train_data, size = 1, maxit = 200, decay = 0.001) {

  train_data$y_train <- as.factor(train_data$y_train)

  model <- nnet(y_train ~ ., 
                data = train_data, 
                size = size, 
                maxit = maxit, 
                decay = decay, 
                linout = FALSE, 
                trace=FALSE
                )

  return(model)
}

test <- function(model, test_data){

    y_pred = predict(model, newdata = test_data[, -ncol(test_data)], type = "class")

    accuracy = accuracy <- mean(y_pred == test_data[, ncol(test_data)])
    
    # cat(sprintf("Test Accuracy: %.4f\n", accuracy))
    
    return(accuracy)
}
```

## Genetic Algorithm

```{r}

suppressPackageStartupMessages(library(GA))


train_ga <- function(train_data, size, maxit){
    min_size <- 1
    max_size <- size
    pcrossover <- 0.8
    pmutation <- 0.1
    popSize <- 50

    n_params <- 5 * max_size + (max_size + 1) * 3

    # Set lower and upper bounds for the weights
    lower <- c(min_size, rep(-1, n_params))
    upper <- c(max_size, rep(1, n_params))

    # Set up GA parameters with filled lower and upper bounds
    ga_model <- ga(
        type = "real-valued",
        fitness = fitness_function,
        lower = lower,
        upper = upper,
        pcrossover = pcrossover,
        pmutation = pmutation,
        popSize = popSize,
        maxiter = maxit,
        run = 50,
        monitor = FALSE
)
    # Retrieve optimized weights
    # Returns all of the individuals that are tied for the best fitness score, we only keep the first
    optimal_weights <- ga_model@solution[1, ]

    optimal_size <- round(optimal_weights[1])
    nn_model <- nnet(
        y_train ~ ., 
        data = train_data, 
        size = optimal_size,
        linout = FALSE,
        maxit = 0,
        decay = 0,
        trace = FALSE
    )

    optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

    # Set weights of the neural network
    nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

    return (nn_model)
}


fitness_function <- function(weights) {
    # Instantiate the neural network with the number of neurons given by the first gene
    size <- round(weights[1])
    nn_model <- nnet(
        y_train ~ ., 
        data = train_data, 
        size = size,
        linout = FALSE,
        maxit = 0,
        decay = 0,
        trace = FALSE
    )

    # Set weights of the neural network
    nn_model$wts <- weights[-1]
    
    # Predict on training data
    predictions <- predict(nn_model, as.matrix(train_data[, -ncol(train_data)]), type = "class")
    
    # Calculate misclassification error
    error <- mean(predictions != train_data[, ncol(train_data)])
    
    # Return negative error to maximize fitness function
    return(-error)
}


```
## Evolution Strategy
```{r}

suppressPackageStartupMessages(library(cmaesr))

train_es <- function(train_data, size, maxit){

    min_size <- 1
    max_size <- size
    sigma <- 0.5
    popSize <- 50

    n_params <- 5 * max_size + (max_size + 1) * 3

    lower <- c(min_size, rep(-1, n_params))
    upper <- c(max_size+1, rep(1, n_params))

    obj.fn = makeSingleObjectiveFunction(
        name = "Neural Network Fitness Function",
        fn = function(x) -fitness_function(x), # ES minimizes objective function
        par.set = makeNumericParamSet("weights", len = n_params + 1 , lower = lower, upper = upper)
    )
    
    # Run the ES model
    suppressWarnings(
      es_model <- cmaes(
        obj.fn,
        monitor = makeMonitor(),
        control = list(
          sigma = sigma, # initial step size
          lambda = popSize, # number of offspring
          stop.ons = c(
            list(stopOnMaxIters(maxit)), # stop after maxit iterations
            getDefaultStoppingConditions() # or after default stopping conditions
          )
        )
      )
    )

    # Retrieve optimized weights
    optimal_weights <- es_model$best.param

    optimal_size <- round(optimal_weights[1])
    
    nn_model <- nnet(
        y_train ~ ., 
        data = train_data, 
        size = optimal_size,
        linout = FALSE,
        maxit = 0,
        decay = 0,
        trace = FALSE
    )

    optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

    # Set weights of the neural network
    nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

    return (nn_model)


}

```


## Run

```{r, eval = FALSE}

suppressPackageStartupMessages(library(tictoc))

# ---------- DATA PARAMETERS ----------

hardness_values <- c(0.2, 2)
noise_levels <- c(0.2, 2)
training_sizes <- c(100, 500) # TODO ADD MORE
validation_size <- 1000
test_size <- 1000

# ---------- RUN PARAMETERS ----------

sizes <- c(1, 10, 20)
maxit <- 200
training_functions  <- c("train_bp", "train_ga", "train_es")


# ---------- RESULT STRUCTURE ----------

results <- data.frame(
  hardness = numeric(),
  noise = numeric(),
  training_size = numeric(),
  size = numeric(),
  maxit = numeric(),
  training_function = character(),
  test_accuracy = numeric(),
  training_time = numeric(),
  stringsAsFactors = FALSE
)

# ---------- EVALUATION ----------

for (hardness in hardness_values) {
  for (noise in noise_levels) {
    for (training_size in training_sizes) {

      data <- generate_synthetic_data(training_size, validation_size, test_size, noise, hardness)
      
      train_data <- data$train
      val_data <- data$val
      test_data <- data$test

      for (size in sizes) {
        for (training_function in training_functions) {
            
            # Retrieve the training function by name
            func <- get(training_function)
            
            # Measure training time using tic and toc
            tic() 
            model <- func(train_data, size, maxit)
            training_time <- toc(quiet = TRUE)$toc
            
            # Test accuracy
            test_accuracy <- test(model, test_data)
            
            # Store result
            results <- rbind(results, data.frame(
              hardness = hardness,
              noise = noise,
              training_size = training_size,
              size = size,
              maxit = maxit,
              training_function = training_function,
              test_accuracy = test_accuracy,
              training_time = training_time
            ))
          }
        }
      }
    }
  }

write.csv(results, "results.csv", row.names = FALSE)
```

## Evaluation & results

```{r}

suppressPackageStartupMessages(library(dplyr))

results <- read.csv("results.csv")

# Identify the fastest and most accurate runs
fastest_run <- results[which.min(results$training_time), ]
most_accurate_run <- results[which.max(results$test_accuracy), ]

# Fastest run per training size
fastest_runs <- results %>%
  group_by(training_size) %>%
  filter(training_time == min(training_time)) %>%
  ungroup()

# Most accurate run per hardness and noise
most_accurate_runs <- results %>%
  group_by(hardness, noise) %>%
  filter(test_accuracy == max(test_accuracy)) %>%
  ungroup()

# Fastest Runs per Training Size:
print(fastest_runs)

# Most Accurate Runs per Hardness and Noise:
print(most_accurate_runs)


```

