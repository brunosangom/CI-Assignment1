---
title: "Computational Intelligence - Assignment 1"
author: "Bruno Sánchez and Sheena Lang"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction

This report describes our experiment in training a neural network (NN) using backpropagation, genetic algorithms (GA), and evolutionary strategies (ES), and evaluating these methods on the testing dataset.

We begin by outlining the data collection process, using synthetic data generation based on the real Iris dataset.

Next, we detail each of the three training approaches (backpropagation, GA, and ES) and conduct a hyperparameter search for GA and ES.

Finally, we assess the accuracy and training time of these methods under different synthetic data conditions (hardness, noise, and training size) and present a statistical analysis of the results to identify any significant differences.

## Data Collection

We aim to generate synthetic data based on the Iris dataset, a widely used benchmark in machine learning, to maintain a connection to a real-world problem while still allowing for flexibility. 

By creating synthetic data instead of using the original Iris dataset, we can control the hardness of the classification task and adjust the level of noise, enabling a more flexible experimental setup.

### Analysis Iris Dataset
The Iris dataset consists of flower samples from three different species: Setosa, Versicolor, and Virginica. Each sample includes measurements of four features: sepal length, sepal width, petal length, and petal width.

To ensure that our synthetic data accurately simulates Iris samples, we first perform a preliminary analysis to calculate the feature means and covariances for each of the three species.

The following code chunk calculates and displays these statistics:

```{r}

suppressPackageStartupMessages(library(dplyr))

data <- iris

print(
    data %>%
    group_by(Species) %>%
    summarise(across(where(is.numeric), mean))    
)

cov_matrices <- data %>%
    group_by(Species) %>%
    summarise(cov_matrix = list(cov(select(cur_data(), where(is.numeric))))) %>%
    pull(cov_matrix)

names(cov_matrices) <- unique(data$Species)
print(cov_matrices)

```

### Synthetic Data Generation

By using the means and covariances of the three flower species, we generate synthetic samples by drawing data points from distributions defined by these statistical properties.

To introduce flexibility, we add parameters to control two aspects: *hardness* and *noise* of the data.

- *Hardness* controls the separation between species, with higher values making the classification task easier. Increasing hardness involves expanding the distance between the species’ feature distributions, making them more distinct and easier to classify.

- *Noise* adds random variation to the data, making the classification task more challenging. A higher noise parameter increases overlap between the species’ distributions by introducing variability to feature values. This reduces the distinctiveness of the species and makes it harder for the model to classify them correctly.

These parameters allow us to simulate a range of classification challenges, from well-separated species with minimal noise (easy classification) to highly overlapping distributions with high noise (difficult classification).


```{r}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(GGally))


generate_synthetic_data <- function(n_train, n_val, n_test, noise_level = 0.1, hardness = 0.5) {
  set.seed(123)

  mu_setosa <- c(5.01, 3.43, 1.46, 0.246)
  mu_versicolor <- c(5.94, 2.77, 4.26, 1.33)
  mu_virginica <- c(6.59, 2.97, 5.55, 2.03)

  mu_setosa <- mu_setosa - c(hardness, hardness, hardness, hardness)
  mu_virginica <- mu_virginica + c(hardness, hardness, hardness, hardness)

  setosa_cov <- matrix(c(
    0.12424898, 0.099216327, 0.016355102, 0.010330612,
    0.09921633, 0.143689796, 0.011697959, 0.009297959,
    0.01635510, 0.011697959, 0.030159184, 0.006069388,
    0.01033061, 0.009297959, 0.006069388, 0.011106122
  ), nrow = 4, byrow = TRUE)

  versicolor_cov <- matrix(c(
    0.26643265, 0.08518367, 0.18289796, 0.05577959,
    0.08518367, 0.09846939, 0.08265306, 0.04120408,
    0.18289796, 0.08265306, 0.22081633, 0.07310204,
    0.05577959, 0.04120408, 0.07310204, 0.03910612
  ), nrow = 4, byrow = TRUE)

  virginica_cov <- matrix(c(
    0.40434286, 0.09376327, 0.30328980, 0.04909388,
    0.09376327, 0.10400408, 0.07137959, 0.04762857,
    0.30328980, 0.07137959, 0.30458776, 0.04882449,
    0.04909388, 0.04762857, 0.04882449, 0.07543265
  ), nrow = 4, byrow = TRUE)

  X_setosa_train <- mvrnorm(n_train %/% 3, mu_setosa, setosa_cov)
  X_setosa_val <- mvrnorm(n_val %/% 3, mu_setosa, setosa_cov)
  X_setosa_test <- mvrnorm(n_test %/% 3, mu_setosa, setosa_cov)

  X_versicolor_train <- mvrnorm(n_train %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_val <- mvrnorm(n_val %/% 3, mu_versicolor, versicolor_cov)
  X_versicolor_test <- mvrnorm(n_test %/% 3, mu_versicolor, versicolor_cov)

  X_virginica_train <- mvrnorm(n_train %/% 3, mu_virginica, virginica_cov)
  X_virginica_val <- mvrnorm(n_val %/% 3, mu_virginica, virginica_cov)
  X_virginica_test <- mvrnorm(n_test %/% 3, mu_virginica, virginica_cov)

  X_train <- rbind(X_setosa_train, X_versicolor_train, X_virginica_train)
  y_train <- c(rep(0, n_train %/% 3), rep(1, n_train %/% 3), rep(2, n_train %/% 3))

  X_val <- rbind(X_setosa_val, X_versicolor_val, X_virginica_val)
  y_val <- c(rep(0, n_val %/% 3), rep(1, n_val %/% 3), rep(2, n_val %/% 3))

  X_test <- rbind(X_setosa_test, X_versicolor_test, X_virginica_test)
  y_test <- c(rep(0, n_test %/% 3), rep(1, n_test %/% 3), rep(2, n_test %/% 3))

  X_train <- X_train + rnorm(length(X_train), 0, noise_level)
  X_val <- X_val + rnorm(length(X_val), 0, noise_level)
  X_test <- X_test + rnorm(length(X_test), 0, noise_level)

  list(
    train = data.frame(X_train, y_train = as.factor(y_train)),
    val = data.frame(X_val, y_val = as.factor(y_val)),
    test = data.frame(X_test, y_test = as.factor(y_test))
  )
}

# ---------- DATA PARAMETERS ----------

# hardness_values <- c(0.2, 2)
# noise_levels <- c(0.2, 2)
# training_sizes <- c(100, 500)
# validation_size <- 1000
# test_size <- 1000
 
# for (training_size in training_sizes){
#     for (noise_level in noise_levels) {
#         for (hardness in hardness_values) {
#             synthetic_data <- generate_synthetic_data(training_size, validation_size, test_size, noise_level, hardness)
#             plot <-ggpairs(synthetic_data$train, aes(color = y_train)) +
#             ggtitle(paste("Pair Plot of Synthetic Training Data (Noise:", noise_level, ", Hardness:", hardness, ")"))
#             print(plot)
#   }
# }
# }
```

## Neural Network

In this section, we utilize the *nnet* package to implement a training function that utilizes backpropagation for our neural network by fitting a model to the training data, specifying parameters such as the number of units in the hidden layer and decay rate.

Additionally, we define a test function to evaluate our neural network on the testing data, assessing its predictive accuracy. 

This testing function is employed not only for evaluating the training using backpropagation, as defined here, but also for assessing models trained with genetic algorithms and evolutionary strategies.

```{r}
suppressPackageStartupMessages(library(nnet))

train_bp <- function(train_data, size, maxit, specific_params) {
  decay <- specific_params$decay
  train_data$y_train <- as.factor(train_data$y_train)

  model <- nnet(y_train ~ .,
    data = train_data,
    size = size,
    maxit = maxit,
    decay = decay,
    linout = FALSE,
    trace = FALSE
  )

  return(model)
}

test <- function(model, test_data) {
  y_pred <- predict(model, newdata = test_data[, -ncol(test_data)], type = "class")

  accuracy <- accuracy <- mean(y_pred == test_data[, ncol(test_data)])

  # cat(sprintf("Test Accuracy: %.4f\n", accuracy))

  return(accuracy)
}
```

## Genetic Algorithm
In this section, we set up the iterative process of training a neural network using a Genetic Algorithm (GA) instead of back-propagation, through the use of the *GA* library.
In order to do this, we iteratively evaluate candidate neural networks (out of a population) according their classification performance on the training data.
Additionally, we include as an evolvable parameter the number of neurons of the neural network, limited by a maximum size (for which we will later evaluate different values).

The training process of the Neural Network with the Genetic Algorithm is as follows:
1. Generate a population where each individual has a random number of neurons (an integer between 1 and the maximum size) and a random sets of weights (each a real number between -1 and 1).
2. For each individual of the population, build a neural network with the corresponding number of neurons and set of weights. This is a candidate neural network.
3. With each candidate neural network, make predictions on the training set and, since GAs maximize the fitness function, return the opposite of the misclassification error as its fitness score.
4. Once the GA is finished, extract the individual with the best fitness and return the neural network associated with it.

This process is implemented in the following code:

```{r}
suppressPackageStartupMessages(library(GA))


train_ga <- function(train_data, size, maxit, specific_params) {
  pcrossover <- specific_params$pcrossover
  pmutation <- specific_params$pmutation
  popSize <- specific_params$pop_size_ga

  min_size <- 1
  max_size <- size

  n_params <- 5 * max_size + (max_size + 1) * 3

  # Set lower and upper bounds for the weights
  lower <- c(min_size, rep(-1, n_params))
  upper <- c(max_size, rep(1, n_params))

  # Set up GA parameters with filled lower and upper bounds
  ga_model <- ga(
    type = "real-valued",
    fitness = fitness_function,
    lower = lower,
    upper = upper,
    pcrossover = pcrossover,
    pmutation = pmutation,
    popSize = popSize,
    maxiter = maxit,
    run = 50,
    monitor = FALSE
  )
  # Retrieve optimized weights
  # Returns all of the individuals that are tied for the best fitness score, we only keep the first
  optimal_weights <- ga_model@solution[1, ]

  optimal_size <- round(optimal_weights[1])
  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = optimal_size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

  # Set weights of the neural network
  nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

  return(nn_model)
}


fitness_function <- function(weights) {
  # Instantiate the neural network with the number of neurons given by the first gene
  size <- round(weights[1])
  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  # Set weights of the neural network
  nn_model$wts <- weights[-1]

  # Predict on training data
  predictions <- predict(nn_model, as.matrix(train_data[, -ncol(train_data)]), type = "class")

  # Calculate misclassification error
  error <- mean(predictions != train_data[, ncol(train_data)])

  # Return negative error to maximize fitness function
  return(-error)
}
```

## Hyperparameter Search GA

The genetic algorithm training function employs three key parameters: 
p-crossover, which represents the probability of combining genes from two parent solutions to create offspring; 
p-mutation, which indicates the likelihood of introducing random changes to a solution to maintain genetic diversity; and 
popsize, which defines the total number of candidate solutions in each generation. 

Since we are modifying parameters of the data such as noise, hardness, and training size, we decided to keep the GA parameters constant.
To establish these constant parameters, we conducted a small hyperparameter search using a training size of 200, a hardness of 0.5, and a noise of 0.5.
We will vary these values in our evaluation, but for simplicity, we chose these parameters as they represent a middle ground within the ranges we will assess later. 
By running the code chunk below, we identified the optimal parameters: a crossover probability of 0.7, a mutation probability of 0.2, and a population size of 100.


```{r, eval=FALSE}
hyperparam_search_ga <- function(train_data, validation_data, pcrossover_values, pmutation_values, popSize_values) {
  print("Running hyperparameter search ga...")

  best_model <- NULL
  best_score <- -Inf
  best_params <- list()

  for (pcrossover in pcrossover_values) {
    for (pmutation in pmutation_values) {
      for (popSize in popSize_values) {
        specific_params <- list(pcrossover = pcrossover, pmutation = pmutation, pop_size_ga = popSize)

        model <- train_ga(train_data, size, maxit, specific_params)

        score <- test(model, validation_data)

        if (score > best_score) {
          best_score <- score
          best_model <- model
          best_params <- list(pcrossover = pcrossover, pmutation = pmutation, popSize = popSize)
        }

        cat("Tested pcrossover =", pcrossover, "pmutation =", pmutation, "popSize =", popSize, "-> score:", score, "\n")
      }
    }
  }
  best_p_crossover_ga <- best_params$pcrossover
  best_p_mutation_ga <- best_params$pmutation
  best_pop_size_ga <- best_params$popSize


  cat("Best pcrossover:", best_p_crossover_ga, "Best pmutation:", best_p_mutation_ga, "Best popsize:", best_pop_size_ga, "Best score:", best_score, "\n")

  return(list(best_model = best_model, best_score = best_score, best_params = best_params))
}

data <- generate_synthetic_data(200, 200, 200, 0.5, 0.5)
train_data <- data$train
validation_data <- data$val

size <- 10
maxit <- 200

# Define possible hyperparameter ranges
pcrossover_values <- c(0.7, 0.8, 0.9)
pmutation_values <- c(0.05, 0.1, 0.2)
popSize_values <- c(20, 50, 100)

best_result <- hyperparam_search_ga(train_data, validation_data, pcrossover_values, pmutation_values, popSize_values)

best_pcrossover_ga <- best_result$best_params$pcrossover
best_pmutation_ga <- best_result$best_params$pmutation
best_pop_size_ga <- best_result$best_params$popSize
```

```{r, echo=FALSE}
# The following values are obtained by running the hyperparameter search for GA.
# In order to avoid running this chunk multiple times, we decided to hard-code the results of the search.

best_pcrossover_ga <- 0.7
best_pmutation_ga <- 0.2
best_pop_size_ga <- 100
```

## Evolution Strategy
In this section, we set up the iterative process of training a neural network using an Evolutionary Strategy (ES) instead of back-propagation, employing the *cmaesr* library.
This process is essentially the same as the one we perform with the Genetic Algorithm, with some key changes:
- We employ an ES instead of a GA for the evolution of the population. In addition to adapting the candidate neural networks, an ES also evolves the mutation parameters themselves separately for each individual.
- An ES minimizes the objective function, so we utilize the opposite of the previous GA fitness function as objective (i.e. the misclassification error). 

```{r}
suppressPackageStartupMessages(library(cmaesr))

# TODO modify optional param values
train_es <- function(train_data, size, maxit, specific_params) {
  sigma <- specific_params$sigma
  popSize <- specific_params$pop_size_es
  min_size <- 1
  max_size <- size

  n_params <- 5 * max_size + (max_size + 1) * 3

  lower <- c(min_size, rep(-1, n_params))
  upper <- c(max_size + 1, rep(1, n_params))

  obj.fn <- makeSingleObjectiveFunction(
    name = "Neural Network Fitness Function",
    fn = function(x) -fitness_function(x), # ES minimizes objective function
    par.set = makeNumericParamSet("weights", len = n_params + 1, lower = lower, upper = upper)
  )

  # Run the ES model
  suppressWarnings(
    es_model <- cmaes(
      obj.fn,
      monitor = makeMonitor(),
      control = list(
        sigma = sigma, # initial step size
        lambda = popSize, # number of offspring
        stop.ons = c(
          list(stopOnMaxIters(maxit)), # stop after maxit iterations
          getDefaultStoppingConditions() # or after default stopping conditions
        )
      )
    )
  )

  # Retrieve optimized weights
  optimal_weights <- es_model$best.param

  optimal_size <- round(optimal_weights[1])

  nn_model <- nnet(
    y_train ~ .,
    data = train_data,
    size = optimal_size,
    linout = FALSE,
    maxit = 0,
    decay = 0,
    trace = FALSE
  )

  optimal_n_params <- 5 * optimal_size + (optimal_size + 1) * 3

  # Set weights of the neural network
  nn_model$wts <- optimal_weights[2:(optimal_n_params + 1)]

  return(nn_model)
}
```

## Hyperparameter Search ES
Similarly to the hyperparameter search conducted for the genetic algorithm (GA), we also perform a hyperparameter search for evolutionary strategies (ES) to identify the optimal values for \sigma, which represents the standard deviation used to control the mutation step size, and the population size, which defines the number of candidate solutions evaluated in each generation. 
We utilize the same data configuration as for the GA hyperparameter tuning to ensure consistency. 
By running the code chunk below, we determined the best \sigma to be 0.4 and the best population size to be 70.

```{r, eval=FALSE}
hyperparam_search_es <- function(train_data, validation_data, sigma_values, popSize_values) {
  best_model <- NULL
  best_score <- -Inf
  best_params <- list()

  for (sigma in sigma_values) {
    for (popSize in popSize_values) {
      specific_params <- list(sigma = sigma, pop_size_es = popSize)
      model <- train_es(train_data, size, maxit, specific_params)

      score <- test(model, validation_data)

      if (score > best_score) {
        best_score <- score
        best_model <- model
        best_params <- list(sigma = sigma, popSize = popSize)
      }

      cat("Tested sigma =", sigma, "popSize =", popSize, "-> score:", score, "\n")
    }
  }

  best_sigma_es <- best_params$sigma
  best_pop_size_es <- best_params$popSize

  cat("Best sigma:", best_sigma_es, "Best popSize:", best_pop_size_es, "Best score:", best_score, "\n")

  return(list(best_model = best_model, best_score = best_score, best_params = best_params))
}

data <- generate_synthetic_data(200, 200, 200, 0.5, 0.5)
train_data <- data$train
validation_data <- data$val

size <- 10
maxit <- 200

# Define possibel hyperparameter ranges
sigma_values <- c(0.4, 0.5, 0.6)
popSize_values <- c(20, 50, 70)

best_result <- hyperparam_search_es(train_data, validation_data, sigma_values, popSize_values)

best_sigma_es <- best_result$best_params$sigma
best_pop_size_es <- best_result$best_params$popSize
```


```{r, echo=FALSE}
# The following values are obtained by running the hyperparameter search for ES.
# In order to avoid running this chunk multiple times, we decided to hard-code the results of the search.

best_sigma_es <- 0.4
best_pop_size_es <- 70
```

## Experimental Setup 
In the experimental setup, we specify various data configurations, namely, hardness, noise, and training sizes. 
Additionally we vary numbers of hidden neurons (sizes). For each training approach (backpropagation, GA and ES), we evaluate performance by tracking both training times and testing accuracies.

```{r, eval = FALSE}
suppressPackageStartupMessages(library(tictoc))

# ---------- DATA PARAMETERS ----------

hardness_values <- c(0.2, 2)
noise_levels <- c(0.2, 2)
training_sizes <- c(100, 500) # TODO ADD MORE
validation_size <- 1000
test_size <- 1000

# ---------- RUN PARAMETERS ----------

sizes <- c(1, 10, 20)
maxit <- 200
training_functions <- c("train_bp", "train_ga", "train_es")
specific_params <- list(decay = 0.001, pcrossover = best_pcrossover_ga, pmutation = best_pmutation_ga, pop_size_ga = best_pop_size_ga, sigma = best_sigma_es, pop_size_es = best_pop_size_es)


# ---------- RESULT STRUCTURE ----------

results <- data.frame(
  hardness = numeric(),
  noise = numeric(),
  training_size = numeric(),
  size = numeric(),
  maxit = numeric(),
  training_function = character(),
  test_accuracy = numeric(),
  training_time = numeric(),
  stringsAsFactors = FALSE
)

# ---------- EVALUATION ----------

for (hardness in hardness_values) {
  for (noise in noise_levels) {
    for (training_size in training_sizes) {
      data <- generate_synthetic_data(training_size, validation_size, test_size, noise, hardness)

      train_data <- data$train
      val_data <- data$val
      test_data <- data$test

      for (size in sizes) {
        for (training_function in training_functions) {
          # Retrieve the training function by name
          func <- get(training_function)

          # Measure training time using tic and toc
          tic()
          model <- func(train_data = train_data, size = size, maxit = maxit, specific_params = specific_params)
          training_time <- toc(quiet = TRUE)$toc

          # Test accuracy
          test_accuracy <- test(model, test_data)

          # Store result
          results <- rbind(results, data.frame(
            hardness = hardness,
            noise = noise,
            training_size = training_size,
            size = size,
            training_function = training_function,
            test_accuracy = test_accuracy,
            training_time = training_time
          ))
        }
      }
    }
  }
}

write.csv(results, "results.csv", row.names = FALSE)
```

## Results
In this section, we observe and analyze the results attained in the previous experiment.

### Results Overview
We first perform a preliminary observation of the results. We extract the fastest run per training size, and the most accurate run per hardness and noise values.

```{r}

results <- read.csv("results.csv")

# Fastest run per training size
fastest_runs <- results %>%
  group_by(training_size) %>%
  filter(training_time == min(training_time)) %>%
  ungroup()

fastest_runs <- fastest_runs %>% select(-test_accuracy)


# Most accurate run per hardness and noise
most_accurate_runs <- results %>%
  group_by(hardness, noise) %>%
  filter(test_accuracy == max(test_accuracy)) %>%
  ungroup()

most_accurate_runs <- most_accurate_runs %>% select(-training_time)

# Fastest Runs per Training Size:
print(fastest_runs)

# Most Accurate Runs per Hardness and Noise:
print(most_accurate_runs)

```

First, we see that the BP method seems to achieve the best training times for both training sizes; with an expected significant increase in time between the 2 sets.

Then, we observe that:
- For the data that most resembles the original Iris dataset (low hardness and noise values), the ES achieves the best accuracy score (0.974).
- For the hardest dataset configuration (low hardness and high noise), the highest accuracy score (0.652) is very moderate, and achieved by BP.
- For the easiest configuration (high hardness, low noise), several models achieve a perfect accuracy score of 1, since the data is easily separable.
- For the final configuration (high hardness and noise), the BP again gets the best score (0.899), with only 1 neuron.

### Statistical Analysis
Let us now perform an statistical analysis on the results obtained for the different training methods.
The goal of this analysis is to find statistically significant differences among the accuracy scores and times of the 3 training methods.

```{r, echo = FALSE}

# Load required libraries
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(car))
suppressPackageStartupMessages(library(FSA))
suppressPackageStartupMessages(library(rstatix))
suppressPackageStartupMessages(library(PMCMRplus))

# Function to write to file with console mirroring
write_and_print <- function(text, file_conn) {
  cat(text)
  cat(text, file = file_conn, append = TRUE)
}

# Start analysis and create output file
analyze_and_report <- function(alpha = 0.15) {  # Added alpha parameter with default value
  # Create output file
  output_file <- "statistical_analysis_report.txt"
  file_conn <- file(output_file, "w")
  
  write_and_print("Statistical Analysis Report for Training Functions\n", file_conn)
  write_and_print("==============================================\n\n", file_conn)
  write_and_print(sprintf("Using significance level (alpha) = %.3f\n\n", alpha), file_conn)
  
  # Read the data
  data <- read_csv("results.csv", show_col_types = FALSE)
  data$training_function <- factor(data$training_function)
  
  # Function to check assumptions and write explanation
  check_assumptions <- function(data, dv) {
    write_and_print(sprintf("\nChecking Statistical Assumptions for %s\n", dv), file_conn)
    write_and_print("----------------------------------------\n", file_conn)
    
    # Shapiro-Wilk test explanation
    write_and_print("\nPerforming Shapiro-Wilk test for normality:\n", file_conn)
    write_and_print("H0: Data is normally distributed\n", file_conn)
    write_and_print("H1: Data is not normally distributed\n\n", file_conn)
    
    normal_tests <- data %>%
      group_by(training_function) %>%
      summarise(
        shapiro_stat = shapiro.test(get(dv))$statistic,
        shapiro_p = shapiro.test(get(dv))$p.value
      )
    
    print(normal_tests)
    capture.output(print(normal_tests), file = file_conn, append = TRUE)
    
    # Interpret normality results
    write_and_print("\nNormality Test Interpretation:\n", file_conn)
    for(i in 1:nrow(normal_tests)) {
      result_text <- sprintf("- %s: %s (p = %.4f)\n",
                           normal_tests$training_function[i],
                           ifelse(normal_tests$shapiro_p[i] >= alpha, 
                                  "Normally distributed",
                                  "Not normally distributed"),
                           normal_tests$shapiro_p[i])
      write_and_print(result_text, file_conn)
    }
    
    # Levene's test explanation and execution
    write_and_print("\nPerforming Levene's test for homogeneity of variance:\n", file_conn)
    write_and_print("H0: Variances are equal across groups\n", file_conn)
    write_and_print("H1: Variances are not equal across groups\n\n", file_conn)
    
    levene_test <- leveneTest(as.formula(paste(dv, "~ training_function")), data = data)
    print(levene_test)
    capture.output(print(levene_test), file = file_conn, append = TRUE)
    
    # Interpret Levene's test
    levene_p <- levene_test$`Pr(>F)`[1]
    result_text <- sprintf("\nVariance Test Interpretation: %s (p = %.4f)\n",
                         ifelse(levene_p >= alpha,
                                "Equal variances across groups",
                                "Unequal variances across groups"),
                         levene_p)
    write_and_print(result_text, file_conn)
    
    # Return whether parametric tests are appropriate
    all_normal <- all(normal_tests$shapiro_p >= alpha)
    vars_homogeneous <- levene_p >= alpha
    
    write_and_print(sprintf("\nBased on these results, %s tests will be used.\n",
                          ifelse(all_normal && vars_homogeneous,
                                 "parametric",
                                 "non-parametric")), file_conn)
    
    return(all_normal && vars_homogeneous)
  }
  
  # Function to analyze a variable
  analyze_variable <- function(data, dv) {
    write_and_print(sprintf("\n\nAnalysis of %s\n", dv), file_conn)
    write_and_print("======================\n", file_conn)
    
    use_parametric <- check_assumptions(data, dv)
    
    if(use_parametric) {
      # ANOVA
      write_and_print("\nPerforming One-way ANOVA:\n", file_conn)
      write_and_print("H0: All group means are equal\n", file_conn)
      write_and_print("H1: At least one group mean is different\n\n", file_conn)
      
      aov_result <- aov(as.formula(paste(dv, "~ training_function")), data = data)
      print(summary(aov_result))
      capture.output(print(summary(aov_result)), file = file_conn, append = TRUE)
      
      # Interpret ANOVA
      aov_p <- summary(aov_result)[[1]]["training_function", "Pr(>F)"]
      result_text <- sprintf("\nANOVA Interpretation: %s (p = %.4f)\n",
                           ifelse(aov_p < alpha,
                                  "There are significant differences between groups",
                                  "No significant differences between groups"),
                           aov_p)
      write_and_print(result_text, file_conn)
      
      # Post-hoc tests
      if(aov_p < alpha) {
        write_and_print("\nPerforming Tukey's HSD test for pairwise comparisons:\n", file_conn)
        tukey_result <- TukeyHSD(aov_result)
        print(tukey_result)
        capture.output(print(tukey_result), file = file_conn, append = TRUE)
      }
      
    } else {
      # Kruskal-Wallis
      write_and_print("\nPerforming Kruskal-Wallis test:\n", file_conn)
      write_and_print("H0: All groups have the same distribution\n", file_conn)
      write_and_print("H1: At least one group has a different distribution\n\n", file_conn)
      
      kw_result <- kruskal.test(as.formula(paste(dv, "~ training_function")), data = data)
      print(kw_result)
      capture.output(print(kw_result), file = file_conn, append = TRUE)
      
      # Interpret Kruskal-Wallis
      result_text <- sprintf("\nKruskal-Wallis Test Interpretation: %s (p = %.4f)\n",
                           ifelse(kw_result$p.value < alpha,
                                  "There are significant differences between groups",
                                  "No significant differences between groups"),
                           kw_result$p.value)
      write_and_print(result_text, file_conn)
      
      # Post-hoc tests if significant
      if(kw_result$p.value < alpha) {
                write_and_print("\nPerforming Dunn's test for pairwise comparisons:\n", file_conn)
        write_and_print("H0: No difference between the pair of groups\n", file_conn)
        write_and_print("H1: Significant difference exists between the pair of groups\n\n", file_conn)
        
        dunn_result <- dunnTest(as.formula(paste(dv, "~ training_function")), 
                               data = data,
                               method = "bh")  # Using Benjamini-Hochberg adjustment
        print(dunn_result)
        capture.output(print(dunn_result), file = file_conn, append = TRUE)

        # Add interpretation of Dunn's test results
        write_and_print("\nDunn's Test Interpretation:\n", file_conn)
        write_and_print("-------------------------\n", file_conn)
        
        # Extract comparisons and p-values
        comparisons <- dunn_result$res
        
        # Sort by p-value for clearer presentation
        comparisons <- comparisons[order(comparisons$P.adj), ]
        
        for (i in 1:nrow(comparisons)) {
            comparison <- comparisons[i, ]
            sig_status <- if (comparison$P.adj < alpha) {
                "Significant difference"
            } else {
                "No significant difference"
            }

            # Properly parse the comparison string
            groups <- strsplit(as.character(comparison$Comparison), " - ")[[1]]
            group1 <- groups[1]
            group2 <- groups[2]

            # Calculate means with proper error handling
            mean1 <- tryCatch(
                {
                    mean(data[[dv]][data$training_function == group1], na.rm = TRUE)
                },
                error = function(e) NA
            )

            mean2 <- tryCatch(
                {
                    mean(data[[dv]][data$training_function == group2], na.rm = TRUE)
                },
                error = function(e) NA
            )

            # Determine which group performed better with NA handling
            better_group <- if (is.na(mean1) || is.na(mean2)) {
                "Unable to determine performance difference"
            } else if (mean1 > mean2) {
                sprintf(
                    "%s (%.3f) performed better than %s (%.3f)",
                    group1, mean1, group2, mean2
                )
            } else if (mean2 > mean1) {
                sprintf(
                    "%s (%.3f) performed better than %s (%.3f)",
                    group2, mean2, group1, mean1
                )
            } else {
                sprintf(
                    "%s and %s performed equally (%.3f)",
                    group1, group2, mean1
                )
            }

            result_text <- sprintf(
                "- %s vs %s: %s (p = %.4f)\n  %s\n",
                group1, group2,
                sig_status,
                comparison$P.adj,
                better_group
            )
            write_and_print(result_text, file_conn)
        }
        
        # Add overall summary of significant differences
        sig_pairs <- sum(comparisons$P.adj < alpha)
        total_pairs <- nrow(comparisons)
        write_and_print(sprintf("\nSummary: Found significant differences in %d out of %d comparisons.\n",
                              sig_pairs, total_pairs), file_conn)
      }
    }
  }
  
  # Perform analyses
  analyze_variable(data, "test_accuracy")
  analyze_variable(data, "training_time")
  
  # Summary statistics
  write_and_print("\n\nSummary Statistics\n", file_conn)
  write_and_print("=================\n", file_conn)
  
  summary_stats <- data %>%
    group_by(training_function) %>%
    summarise(
      n = n(),
      mean_accuracy = mean(test_accuracy),
      sd_accuracy = sd(test_accuracy),
      mean_time = mean(training_time),
      sd_time = sd(training_time)
    )
  print(summary_stats)
  capture.output(print(summary_stats), file = file_conn, append = TRUE)

  write_and_print("\n\nAverage Test Accuracy:\n", file_conn)
  
  acc_means <- summary_stats$mean_accuracy
  names(acc_means) <- summary_stats$training_function
  best_acc <- names(which.max(acc_means))
  worst_acc <- names(which.min(acc_means))
  
  write_and_print(sprintf("- Best performing method: %s (%.3f)\n", 
                        best_acc, max(acc_means)), file_conn)
  write_and_print(sprintf("- Worst performing method: %s (%.3f)\n", 
                        worst_acc, min(acc_means)), file_conn)
  
  write_and_print("\nAverage Training Time:\n", file_conn)
  time_means <- summary_stats$mean_time
  names(time_means) <- summary_stats$training_function
  fastest <- names(which.min(time_means))
  slowest <- names(which.max(time_means))
  
  write_and_print(sprintf("- Fastest method: %s (%.2f)\n", 
                        fastest, min(time_means)), file_conn)
  write_and_print(sprintf("- Slowest method: %s (%.2f)\n", 
                        slowest, max(time_means)), file_conn)
  
  # Close the connection
  close(file_conn)
  cat("\nReport has been saved to", output_file, "\n")
}

```

*Remark:* The code for the statistical analysis is hidden in the final report, but can be visualized in the attached Rmarkdown file. This code was generated by the Claude 3.5 Sonnet LLM model (Anthropic AI, 2024).  

```{r}

# Run the statistical analysis with alpha = 0.15
alpha <- 0.15
analyze_and_report(alpha)

```

With a significance level of 0.15, we can conclude from these results that the GA training method performs significantly worse in accuracy than both BP and ES methods. While ES achieved slightly higher accuracy than BP on average, this difference is likely insignificant and could be due to random variation. In terms of training time, no significant differences were found among the different methods.

## Conclusion

